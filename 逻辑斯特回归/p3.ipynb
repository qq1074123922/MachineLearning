{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑斯特回归模型（peter）\n",
    "线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#包含各种头部文件\n",
    "# 包含头部文件(科学计算包)\n",
    "import numpy as np\n",
    "#包含数据分析包\n",
    "import pandas as pd\n",
    "#包含绘图包matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#包含scipy包\n",
    "from scipy.optimize import minimize\n",
    "#包含sklearn包\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#初始化matplotlib的参数\n",
    "#将matplotlib绘制的图标显示在Notebook中\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义文件导入方法 \n",
    "'''\n",
    "@file - 文件名（包含文件路径）\n",
    "@delimeter - 文件中特征的分隔符 \n",
    "'''\n",
    "def loaddata(file, delimeter):\n",
    "    #载入文本数据 np.loadtxt \n",
    "    data = np.loadtxt(file, delimiter=delimeter)\n",
    "    #显示数据的维度、\n",
    "    print \"dimension:\", data.shape\n",
    "    #显示数据的前六行, 所有列\n",
    "    print data[:6, :]\n",
    "    #返回data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-58-bf5ce0bc0b1b>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-58-bf5ce0bc0b1b>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    neg = data[:, 2] == 0\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#定义绘图的方法\n",
    "'''\n",
    "@data\n",
    "@label_x\n",
    "@label_y\n",
    "@label_pos\n",
    "@label_neg\n",
    "@axes\n",
    "'''\n",
    "def plotdata(data, label_x, label_y, label_pos, label_neg, axes=None):b\n",
    "    #获得负样本的下标\n",
    "    neg = data[:, 2] == 0\n",
    "    #获得正样本的下标\n",
    "    pos = data[:, 2] == 1\n",
    "    #指明采用何种axes\n",
    "    if axes == None:\n",
    "        axes = plt.gca() #采用当前axes\n",
    "    #在当前的axes上进行绘图\n",
    "    #绘制正样本的散点图\n",
    "    axes.scatter(data[pos][:,0], data[pos][:,1], marker='+',c='k', s=60, linewidth=2, label=label_pos)\n",
    "    #绘制负样本的散点图\n",
    "    axes.scatter(data[neg][:,0], data[neg][:, 1],marker='o',c='y',  s=60, linewidth=2, label=label_neg)\n",
    "    #设置x轴的标签\n",
    "    axes.set_xlabel(label_x)\n",
    "    #设置y轴的标签\n",
    "    axes.set_ylabel(label_y)\n",
    "    #在frame上画图，同时采用fancybox\n",
    "    axes.legend(frameon=True, fancybox=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义sigmoid函数\n",
    "def sigmoid(z):\n",
    "    r = (1 / (1 + np.exp(-z)))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 损失函数\n",
    "#### $$ J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\big[-y^{(i)}\\, log\\,( h_\\theta\\,(x^{(i)}))-(1-y^{(i)})\\,log\\,(1-h_\\theta(x^{(i)}))\\big]$$\n",
    "#### 向量化的损失函数(矩阵形式)\n",
    "#### $$ J(\\theta) = \\frac{-1}{m}\\big((\\,log\\,(g(X\\theta))^Ty+(\\,log\\,(1-g(X\\theta))^T(1-y)\\big)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义损失函数\n",
    "'''\n",
    "@theta -- 权重\n",
    "@X -- 样本集的所有特征矩阵\n",
    "@y -- 样本集的结果列向量\n",
    "'''\n",
    "def costFunction(theta, X, y):\n",
    "    # m - 样本总数\n",
    "    m = y.shape[0] # 取行的值\n",
    "    \n",
    "    # g - g(X*theta)\n",
    "    g = sigmoid(X.dot(theta))\n",
    "    \n",
    "    # 损失值\n",
    "    cost = (-1) * (1.0 / m) * (np.log(g).T.dot(y) + np.log(1 - g).T.dot(1 - y))\n",
    "\n",
    "    if np.isnan(cost[0]):\n",
    "        return np.inf\n",
    "    \n",
    "    return cost[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 求偏导(梯度)\n",
    "\n",
    "#### $$ \\frac{\\delta J(\\theta)}{\\delta\\theta_{j}} = \\frac{1}{m}\\sum_{i=1}^{m} ( h_\\theta (x^{(i)})-y^{(i)})x^{(i)}_{j} $$ \n",
    "#### 向量化的偏导(梯度)\n",
    "#### $$ \\frac{\\delta J(\\theta)}{\\delta\\theta_{j}} = \\frac{1}{m} X^T(g(X\\theta)-y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义梯度函数\n",
    "'''\n",
    "@theta -- 权重\n",
    "@X     -- 样本集的所有特征的矩阵\n",
    "@y     -- 样本集的结果列向量\n",
    "'''\n",
    "def gradient(theta, X, y):\n",
    "    #样本的总数\n",
    "    m = y.shape[0]\n",
    "\n",
    "    #sigmoid 函数\n",
    "    g = sigmoid(X.dot(theta.reshape(-1, 1)))\n",
    "    \n",
    "    #梯度值\n",
    "    grad = (1.0 / m) * (X.T).dot(g - y)\n",
    "    \n",
    "    return grad.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#自定义梯度下降函数，但是效果很差。使用scipy的优化minimize效果较好\n",
    "def gradientDescent(theta, X, y):\n",
    "    #梯度下降迭代的次数\n",
    "    maxCycles = 1000000\n",
    "    \n",
    "    #学习率 或者 步长\n",
    "    alpha = 0.01\n",
    "    \n",
    "    #梯度下降\n",
    "    cost = costFunction(theta.reshape(-1, 1), X, y)\n",
    "    for i in range(maxCycles):\n",
    "        theta = theta - alpha * gradient(theta, X, y)\n",
    "        \n",
    "    #返回theta的值\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#以下为核心的调用过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调用文本数据\n",
    "data = loaddata('data1.txt', ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看数据的分布情况，散点图\n",
    "plotdata(data, 'Exam 1 Score', 'Exam 2 Score', 'Pass', 'Fail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#初始化样本集X （注：样本集中的第一列必须全部为1，1是指theat0 的系数恒为1。 其余列为样本集的所有列，不包含最后一列）\n",
    "X = np.c_[np.ones(data.shape[0]), data[:,:2]]\n",
    "#初始化结果集y\n",
    "y = np.c_[data[:,2]]\n",
    "#初始化theta\n",
    "theta = np.c_[np.zeros(X.shape[1])]\n",
    "#初始化损失函数\n",
    "cost = costFunction(theta, X, y)\n",
    " #初始化梯度\n",
    "grad = gradient(theta, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#实施梯度下降\n",
    "#效果较差，使用minimize较好\n",
    "#theta = gradientDescent(theta, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化theta\n",
    "theta = np.c_[np.zeros(X.shape[1])]\n",
    "#初始化损失函数\n",
    "cost = costFunction(theta, X, y)\n",
    "print cost\n",
    "#初始化梯度\n",
    "grad = gradient(theta, X, y)\n",
    "print grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最损失函数进行梯度下降，以期求得最小值\n",
    "#入参\n",
    "#costFunction --  损失函数\n",
    "# theta       --  权重值\n",
    "# args（X，y）--  样本特征集合和正负样本集\n",
    "# jac         --  梯度下降方式\n",
    "# options={‘maxiter’：400} -- 迭代400次\n",
    "#出参\n",
    "# x 也即最佳的权重值， 它为行向量\n",
    "res = minimize(costFunction, theta, args=(X,y), method=None, jac=gradient, options={'maxiter':400})\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义预测函数\n",
    "def predict(theta, X, threshhold=0.5):\n",
    "    #得到预测后的向量P, 为列向量\n",
    "    P = sigmoid(X.dot(theta)) >= threshhold\n",
    "    return P.astype('int') #将true 或者false 转换成为 1 和 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一门课45分，第二门课85分的同学\n",
    "# 咱们对他做个预测，拿到通过考试的概率\n",
    "sigmoid(np.array([1, 45, 85]).dot(res.x.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算整个样本集的预测精度\n",
    "P = predict(res.x.reshape(-1, 1), X)\n",
    "#打印整个训练集的精度\n",
    "print \"Train accuracu {}%\".format(100.0 * sum( P == y)[0] / P.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制决策边界"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#绘制整个数据集的散点图\n",
    "plotdata(data, \"Exam1 score\", \"Exam2 score\", \"Fail\", \"Pass\")\n",
    "#在该散点图中加入奇异值点\n",
    "plt.scatter(45, 85, s=60, c='r', marker='v', label='(45, 85)')\n",
    "#取得第1维的最小值\n",
    "x1_min = X[:,1].min()\n",
    "#取得第1维的最大值\n",
    "x1_max = X[:,1].max()\n",
    "#取得第2维的最小值\n",
    "x2_min = X[:,2].min()\n",
    "#取得第2维的最大值\n",
    "x2_max = X[:,2].max()\n",
    "#生成网格点xx1和xx2\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "#计算sigmoid的值\n",
    "h = sigmoid(np.c_[np.ones((xx1.ravel().shape[0],1)), xx1.ravel().reshape(-1, 1), xx2.ravel().reshape(-1, 1)].dot(res.x.reshape(-1, 1)))\n",
    "#将列向量重新构成与xx1 和 xx2 相同shape的数组\n",
    "h = h.reshape(xx1.shape)\n",
    "#绘制决策边界，取概率值为0.5 的决策边界\n",
    "plt.contour(xx1, xx2, h, [0.5], linewidths=1, colors='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "逻辑斯特回归 非线性模型（增加了正则化项）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载数据集\n",
    "data1 = loaddata(\"data2.txt\", ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#提取样本集的所有特征\n",
    "X = np.c_[data1[:,0:2]] \n",
    "#提取正负样本值\n",
    "y = np.c_[data1[:,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#绘制散点图\n",
    "plotdata(data1, 'Exam1 score', 'Exam2 score','Pass','Fail')\n",
    "#通过绘制可以看到，该散点图具有非线性边界，因此线性逻辑斯特回归并不适用该数据集。\n",
    "#必须采用非线性回归才能找出决策边界。\n",
    "#要进行非线性回归，第一步就是进行特征映射，生成多项式特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#生成多项式特征，最高的次数是6\n",
    "poly = PolynomialFeatures(6)\n",
    "#生成多项式特征的值\n",
    "XX = poly.fit_transform(data1[:, 0:2])\n",
    "#初始化theta的值\n",
    "theta = np.ones(XX.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 带正则化项的损失函数\n",
    "#### $$ J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\big[-y^{(i)}\\, log\\,( h_\\theta\\,(x^{(i)}))-(1-y^{(i)})\\,log\\,(1-h_\\theta(x^{(i)}))\\big] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}$$\n",
    "#### 向量化的损失函数\n",
    "#### $$ J(\\theta) = \\frac{1}{m}\\big((\\,log\\,(g(X\\theta))^Ty+(\\,log\\,(1-g(X\\theta))^T(1-y)\\big) + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#带正则化项的损失函数\n",
    "def costFunctionReg(theta, X, y, Lambda):\n",
    "    #样本的个数\n",
    "    m = y.size\n",
    "    #g(X*theta)的值\n",
    "    g = sigmoid(X.dot(theta.reshape(-1, 1)))\n",
    "    #损失函数的表达式\n",
    "    J = (-1) * (1.0 / m) * (np.log(g).T.dot(y) + (np.log(1 - g).T.dot(1 - y))) + (Lambda / (2.0 * m)) * sum(np.square(theta))\n",
    "    \n",
    "    #返回值\n",
    "    if np.isnan(J[0]):\n",
    "        return np.inf\n",
    "    else:\n",
    "        return J[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 还是偏导(梯度)\n",
    "\n",
    "#### $$ \\frac{\\delta J(\\theta)}{\\delta\\theta_{j}} = \\frac{1}{m}\\sum_{i=1}^{m} ( h_\\theta (x^{(i)})-y^{(i)})x^{(i)}_{j} + \\frac{\\lambda}{m}\\theta_{j}$$ \n",
    "#### 向量化\n",
    "#### $$ \\frac{\\delta J(\\theta)}{\\delta\\theta_{j}} = \\frac{1}{m} X^T(g(X\\theta)-y) + \\frac{\\lambda}{m}\\theta_{j}$$\n",
    "##### $$\\text{Note: 要注意的是参数 } \\theta_{0} \\text{ 是不需要正则化的}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义向量化后的梯度函数\n",
    "def gradientReg(theta, X, y, Lambda):\n",
    "    #样本的个数m\n",
    "    m = y.size\n",
    "    #函数g的值\n",
    "    g = sigmoid(X.dot(theta.reshape(-1, 1)))\n",
    "    #梯度值\n",
    "    grad = (1.0 / m) * (X.T.dot(g - y)) + ( (1.0 * Lambda) / m ) * theta.reshape(-1, 1)\n",
    "    return grad.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 决策边界，咱们分别来看看正则化系数lambda太大太小分别会出现什么情况\n",
    "# Lambda = 0 : 就是没有正则化，这样的话，就过拟合咯\n",
    "# Lambda = 1 : 这才是正确的打开方式\n",
    "# Lambda = 100 : 卧槽，正则化项太激进，导致基本就没拟合出决策边界"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#绘制非线性边界（绘制三个子窗口， 大小为17 * 5）\n",
    "fig, axes = plt.subplots(1,3, sharey = True, figsize=(17,5))\n",
    "#初始化theta\n",
    "theta = np.zeros(XX.shape[1])\n",
    "print theta.shape\n",
    "print XX.shape\n",
    "print y.shape\n",
    "print costFunctionReg(theta, XX, y, 1)\n",
    "print gradientReg(theta, XX, y, 1)\n",
    "#分别计算每种Lambda情况下的拟合情况\n",
    "\n",
    "for i, C in enumerate([0, 1, 100]):\n",
    "    theta = np.zeros(XX.shape[1])\n",
    "    #调用最小化函数，进行损失函数的梯度下降的最小化\n",
    "    res = minimize(costFunctionReg, theta, args=(XX, y, C), method=None, jac=gradientReg, options={\"maxiter\":3000})\n",
    "    print res.x\n",
    "    #绘制第i副图\n",
    "    plotdata(data1, \"Exam1 score\", \"Exam2 score\", \"Pass\", \"Fail\", axes.flatten()[i])\n",
    "    #计算准确率\n",
    "    train_accuracy = 100.0 * (sum(predict(res.x.reshape(-1, 1), XX) == y)[0])/ y.size\n",
    "    #绘制拟合的边界\n",
    "    x1_min = X[:,0].min()\n",
    "    x1_max = X[:,0].max()\n",
    "    x2_min = X[:,1].min()\n",
    "    x2_max = X[:,1].max()\n",
    "    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "    h = sigmoid(poly.fit_transform(np.c_[xx1.ravel(), xx2.ravel()]).dot(res.x.reshape(-1, 1)))\n",
    "    h = h.reshape(xx1.shape)\n",
    "    axes.flatten()[i].contour(xx1, xx2, h,[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], linewidths=1, colors='g')\n",
    "    axes.flatten()[i].set_title('Train accuracy {}% with Lambda = {}'.format(np.round(train_accuracy, decimals=2), C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
